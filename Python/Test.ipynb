{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "\n",
    "import cudf as cu\n",
    "from cuml.neighbors import KNeighborsClassifier\n",
    "from cuml.svm import SVC as cuSVC, LinearSVC as cuLinearSVC\n",
    "from cuml.model_selection import train_test_split as cuTrainTestSplit\n",
    "from cuml import RandomForestClassifier as cuRandomForestClassifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CALCULATE METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getConfusionMatrix(labels: np.ndarray, predictions: np.ndarray):\n",
    "    confusion = dict()\n",
    "    confusion[\"TP\"] = np.sum(labels & predictions)\n",
    "    confusion[\"TN\"] = np.sum(~labels & ~predictions)\n",
    "    confusion[\"FP\"] = np.sum(~labels & predictions)\n",
    "    confusion[\"FN\"] = np.sum(labels & ~predictions)\n",
    "    return confusion\n",
    "\n",
    "def getMetrics(confusion):\n",
    "    metrics = dict()\n",
    "    metrics[\"accuracy\"] = (confusion[\"TP\"] + confusion[\"TN\"]) / (confusion[\"TP\"] + confusion[\"TN\"] + confusion[\"FP\"] + confusion[\"FN\"])\n",
    "    metrics[\"precision\"] = confusion[\"TP\"] / (confusion[\"TP\"] + confusion[\"FP\"])\n",
    "    metrics[\"recall\"] = confusion[\"TP\"] / (confusion[\"TP\"] + confusion[\"FN\"])\n",
    "    metrics[\"f1\"] = 2 * (metrics[\"precision\"] * metrics[\"recall\"]) / (metrics[\"precision\"] + metrics[\"recall\"])\n",
    "    return metrics\n",
    "\n",
    "def printMetrics(confusion):\n",
    "    print(\"Accuracy: \\t\", confusion[\"accuracy\"])\n",
    "    print(\"Precision: \\t\", confusion[\"precision\"])\n",
    "    print(\"Recall: \\t\", confusion[\"recall\"])\n",
    "    print(\"F1: \\t\\t\", confusion[\"f1\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid = pd.read_parquet('covidClean.parquet')\n",
    "\n",
    "toRemove = [\"PATIENT_ID\", \"USMER\", \"SYMPTOMS_DATE\",\n",
    "            \"MEDICAL_UNIT\", \"ADMISSION_DATE\", \"PATIENT_TYPE\",\n",
    "            \"DEATH_DATE\", \"ORIGIN_COUNTRY\"]\n",
    "covid = covid.drop(columns = toRemove)\n",
    "covid = covid.drop(columns= [\"DIED\", \"INTUBED\", \"ICU\"])\n",
    "\n",
    "trainingSet, testSet = train_test_split(covid, test_size = 0.25)\n",
    "\n",
    "trainingY = np.array(trainingSet[\"AT_RISK\"])\n",
    "trainingX = trainingSet.drop(columns = [\"AT_RISK\"])\n",
    "\n",
    "testY = np.array(testSet[\"AT_RISK\"])\n",
    "testX = testSet.drop(columns = [\"AT_RISK\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cuCovid = cu.read_parquet('covidClean.parquet')\n",
    "\n",
    "toRemove = [\"PATIENT_ID\", \"USMER\", \"SYMPTOMS_DATE\",\n",
    "        \"MEDICAL_UNIT\", \"ADMISSION_DATE\", \"PATIENT_TYPE\",\n",
    "        \"DEATH_DATE\", \"ORIGIN_COUNTRY\"]\n",
    "cuCovid = cuCovid.drop(columns = toRemove)\n",
    "cuCovid = cuCovid.drop(columns= [\"DIED\", \"INTUBED\", \"ICU\"])\n",
    "cuCovid = cuCovid.astype(\"float32\")\n",
    "\n",
    "labels = cuCovid[\"AT_RISK\"]\n",
    "covidX = cuCovid.drop(columns = [\"AT_RISK\"])\n",
    "cuTrainingX, cuTestX, cuTrainingY, cuTestY = cuTrainTestSplit(covidX, labels, test_size=0.25)\n",
    "\n",
    "cuTestY = cuTestY.to_pandas().to_numpy().astype(\"bool\")\n",
    "cuTrainY = cuTrainingY.to_pandas().to_numpy().astype(\"bool\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RF metrics (GPU) on Training Set:\n",
      "Accuracy: \t 0.648686646407489\n",
      "Precision: \t 0.6386896920875108\n",
      "Recall: \t 0.7190148339038681\n",
      "F1: \t\t 0.6764761464385517\n",
      "\n",
      "RF metrics (GPU) on Test Set:\n",
      "Accuracy: \t 0.6440626903403696\n",
      "Precision: \t 0.6344711692349901\n",
      "Recall: \t 0.7127117229334943\n",
      "F1: \t\t 0.6713194515842248\n"
     ]
    }
   ],
   "source": [
    "rf = cuRandomForestClassifier(n_estimators=1000)\n",
    "rf.fit(cuTrainingX, cuTrainingY)\n",
    "\n",
    "predictionsTrain = rf.predict(cuTrainingX)\n",
    "predictionsTest = rf.predict(cuTestX)\n",
    "\n",
    "predictionsTrain = predictionsTrain.to_pandas().to_numpy().astype(\"bool\")\n",
    "predictionsTest = predictionsTest.to_pandas().to_numpy().astype(\"bool\")\n",
    "\n",
    "metricsTest = getMetrics(getConfusionMatrix(cuTestY, predictionsTest))\n",
    "metricsTrain = getMetrics(getConfusionMatrix(cuTrainY, predictionsTrain))\n",
    "\n",
    "print(\"\\nRF metrics (GPU) on Training Set:\")\n",
    "printMetrics(metricsTrain)\n",
    "print(\"\\nRF metrics (GPU) on Test Set:\")\n",
    "printMetrics(metricsTest)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NAIVE BAYES GAUSSIAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = GaussianNB()\n",
    "nb.fit(trainingX, trainingY)\n",
    "\n",
    "predictionsTrain = nb.predict(trainingX)\n",
    "predictionTest = nb.predict(testX)\n",
    "\n",
    "metricsTest = getMetrics(getConfusionMatrix(testY, predictionTest))\n",
    "metricsTraing = getMetrics(getConfusionMatrix(trainingY, predictionsTrain))\n",
    "\n",
    "print(\"\\nNB Gaussian metrics on Training Set:\")\n",
    "printMetrics(metricsTraing)\n",
    "print(\"\\nNB Gaussian metrics on Test Set:\")\n",
    "printMetrics(metricsTest)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NAIVE BAYES MULTINOMIAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(trainingX, trainingY)\n",
    "\n",
    "predictionsTrain = nb.predict(trainingX)\n",
    "predictionTest = nb.predict(testX)\n",
    "\n",
    "metricsTest = getMetrics(getConfusionMatrix(testY, predictionTest))\n",
    "metricsTraing = getMetrics(getConfusionMatrix(trainingY, predictionsTrain))\n",
    "\n",
    "print(\"\\nNB Multinomial metrics on Training Set:\")\n",
    "printMetrics(metricsTraing)\n",
    "print(\"\\nNB Multinomial metrics on Test Set:\")\n",
    "printMetrics(metricsTest)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN - K NEAREST NEIGHBORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findBestK(lower, upper, metric=\"accuracy\"):\n",
    "    median = (lower + upper) // 2\n",
    "    if lower == median:\n",
    "        return lower\n",
    "    \n",
    "    results = dict()\n",
    "    for k in [lower, median, upper]:\n",
    "        knn = KNeighborsClassifier(n_neighbors=k)\n",
    "        knn.fit(cuTrainingX, cuTrainingY)\n",
    "        predicted = knn.predict(cuTestX)\n",
    "        \n",
    "        predicted = predicted.to_pandas().to_numpy().astype(\"bool\")\n",
    "        results[k] = getMetrics(getConfusionMatrix(cuTestY, predicted))[metric]\n",
    "\n",
    "    if results[median] > results[lower]:\n",
    "        return findBestK(median, upper, metric)\n",
    "    else:\n",
    "        return findBestK(lower, median, metric)\n",
    "\n",
    "#!----------------------------------------------\n",
    "    \n",
    "bestK = findBestK(1, 1000, \"accuracy\")\n",
    "print(\"Best K: \", bestK)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=bestK)\n",
    "knn.fit(cuTrainingX, cuTrainingY)\n",
    "\n",
    "predictedTrain = knn.predict(cuTrainingX)\n",
    "predictedTest = knn.predict(cuTestX)\n",
    "\n",
    "predictedTrain = predictedTrain.to_pandas().to_numpy().astype(\"bool\")\n",
    "predictedTest = predictedTest.to_pandas().to_numpy().astype(\"bool\")\n",
    "\n",
    "print(\"\\nKNN Metrics on Training Set:\")\n",
    "printMetrics(getMetrics(getConfusionMatrix(cuTrainY, predictedTrain)))\n",
    "\n",
    "print(\"\\nKNN Metrics on Test Set:\")\n",
    "printMetrics(getMetrics(getConfusionMatrix(cuTestY, predictedTest)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN - K NEAREST NEIGHBORS - K = SQRT(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knnSQRT = KNeighborsClassifier(n_neighbors=int(math.sqrt(len(cuTrainingX))))\n",
    "knnSQRT.fit(cuTrainingX, cuTrainingY)\n",
    "\n",
    "predictedTrain = knnSQRT.predict(cuTrainingX)\n",
    "predictedTest = knnSQRT.predict(cuTestX)\n",
    "\n",
    "predictedTrain = predictedTrain.to_pandas().to_numpy().astype(\"bool\")\n",
    "predictedTest = predictedTest.to_pandas().to_numpy().astype(\"bool\")\n",
    "\n",
    "print(\"\\nKNN SQRT Metrics on Training Set:\")\n",
    "printMetrics(getMetrics(getConfusionMatrix(cuTrainY, predictedTrain)))\n",
    "\n",
    "print(\"\\nKNN SQRT Metrics on Test Set:\")\n",
    "printMetrics(getMetrics(getConfusionMatrix(cuTestY, predictedTest)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LINEAR SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = cuLinearSVC(max_iter=100000)\n",
    "svm.fit(cuTrainingX, cuTrainingY)\n",
    "\n",
    "predictionsTrain = svm.predict(cuTrainingX)\n",
    "predictionsTest = svm.predict(cuTestX)\n",
    "\n",
    "predictionsTrain = predictionsTrain.to_pandas().to_numpy().astype(\"bool\")\n",
    "predictionsTest = predictionsTest.to_pandas().to_numpy().astype(\"bool\")\n",
    "\n",
    "metricsTrain = getMetrics(getConfusionMatrix(cuTrainY, predictionsTrain))\n",
    "metricsTest = getMetrics(getConfusionMatrix(cuTestY, predictionsTest))\n",
    "\n",
    "print(\"\\nLinear SVM metrics (GPU) on Training Set:\")\n",
    "printMetrics(metricsTrain)\n",
    "print(\"\\nLinear SVM metrics (GPU) on Test Set:\")\n",
    "printMetrics(metricsTest)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dict()\n",
    "kernels = [\"poly\", \"rbf\", \"sigmoid\"]\n",
    "\n",
    "for kernel in kernels:\n",
    "    svm = cuSVC(kernel=kernel, max_iter=100000)\n",
    "    svm.fit(cuTrainingX, cuTrainingY)\n",
    "\n",
    "    predictionsTrain = svm.predict(cuTrainingX)\n",
    "    predictionsTest = svm.predict(cuTestX)\n",
    "    \n",
    "    predictionsTrain = predictionsTrain.to_pandas().to_numpy().astype(\"bool\")\n",
    "    predictionsTest = predictionsTest.to_pandas().to_numpy().astype(\"bool\")\n",
    "\n",
    "    metricsTrain = getMetrics(getConfusionMatrix(cuTrainY, predictionsTrain))\n",
    "    metricsTest = getMetrics(getConfusionMatrix(cuTestY, predictionsTest))\n",
    "    \n",
    "    print(\"\\n SVM con kernel '\" + kernel + \"' metrics on Training Set: \")\n",
    "    printMetrics(metricsTrain)\n",
    "    print(\"\\n SVM con kernel '\" + kernel + \"' metrics on Test Set: \")\n",
    "    printMetrics(metricsTest)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalMetrics = dict()\n",
    "solvers = [\"lbfgs\", \"liblinear\", \"newton-cg\", \"newton-cholesky\", \"sag\", \"saga\"]\n",
    "\n",
    "for solver in solvers:\n",
    "    lr = LogisticRegression(max_iter=100000, solver=solver)\n",
    "    lr.fit(trainingX, trainingY)\n",
    "    \n",
    "    predictionsTrain = lr.predict(trainingX)\n",
    "    predictionsTest = lr.predict(testX)\n",
    "\n",
    "    metricsTest = getMetrics(getConfusionMatrix(testY, predictionsTest))\n",
    "    metricsTrain = getMetrics(getConfusionMatrix(trainingY, predictionsTrain))\n",
    "    \n",
    "    totalMetrics[solver] = (metricsTrain, metricsTest)\n",
    "\n",
    "    print(\"\\nLogistic Regression metrics (solver = \\\"\" + solver + \"\\\") on Training Set:\")\n",
    "    printMetrics(metricsTrain)\n",
    "    print(\"\\nLogistic Regression metrics (solver = \\\"\" + solver + \"\\\") on Test Set:\")\n",
    "    printMetrics(metricsTest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python-PUM3iCJl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
